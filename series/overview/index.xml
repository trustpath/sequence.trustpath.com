<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview on Sequence</title>
    <link>http://localhost:1313/series/overview/</link>
    <description>Recent content in Overview on Sequence</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Dataence, LLC. All Rights Reserved.</copyright>
    <lastBuildDate>Sat, 28 Feb 2015 18:48:24 -0800</lastBuildDate>
    <atom:link href="http://localhost:1313/series/overview/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introduction</title>
      <link>http://localhost:1313/manual/introduction/</link>
      <pubDate>Sat, 28 Feb 2015 18:48:24 -0800</pubDate>
      
      <guid>http://localhost:1313/manual/introduction/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;#&#34; class=&#34;image fit&#34;&gt;&lt;img src=&#34;http://localhost:1313/images/pic05.jpg&#34; alt=&#34;Obligatory Log Picture&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; is a &lt;em&gt;high performance sequential log analyzer and parser&lt;/em&gt;. It &lt;em&gt;sequentially&lt;/em&gt; goes through a log message, &lt;em&gt;parses&lt;/em&gt; out the meaningful parts, without the use regular expressions. It can achieve &lt;em&gt;high performance&lt;/em&gt; parsing of &lt;strong&gt;100,000 - 200,000 messages per second (MPS)&lt;/strong&gt; without the need to separate parsing rules by log source type.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sequence&lt;/code&gt; is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a set of logs you would like me to test out, please feel free to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open an issue&lt;/a&gt; and we can arrange a way for me to download and test your logs.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;motivation:2767292a573dc549b9b4297b701af3ab&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, analyzing and understanding log messages.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.&lt;/p&gt;

&lt;p&gt;Sequence is developed to make analyzing and parsing log messages a lot easier and faster.&lt;/p&gt;

&lt;h3 id=&#34;existing-approaches:2767292a573dc549b9b4297b701af3ab&#34;&gt;Existing Approaches&lt;/h3&gt;

&lt;p&gt;The industry has came up with a couple of different approaches to solving the log parsing problem. In a way, you can say the log parsing problem has been solved, because analysts have a lot of different tools to choose from when they need to understand logs.&lt;/p&gt;

&lt;p&gt;Commercially, there are companies such as Splunk, ArcSight, Tibco LogLogic, SumoLogic, LogEntries, Loggly, LogRhythm, etc etc etc that can provide you either an on-premise or in-the-cloud (SaaS) solution. They provide different capabilities and feature sets depending on the primary use case you are targetting.&lt;/p&gt;

&lt;p&gt;Open source wise, you have tools such as ElasticSearch, Greylog2, OSSIM, and a few others that wants to provide you end-to-end capabilities similar to the commercial offerings. There are also libraries such as &lt;a href=&#34;http://www.liblognorm.com/&#34;&gt;liblognorm&lt;/a&gt; and &lt;a href=&#34;http://logstash.net/&#34;&gt;logstash&lt;/a&gt; you can use to build your own tools.&lt;/p&gt;

&lt;p&gt;And then there&amp;rsquo;s Fedora&amp;rsquo;s &lt;a href=&#34;https://fedorahosted.org/lumberjack/&#34;&gt;Project Lumberjack&lt;/a&gt;, which &amp;ldquo;is an open-source project to update and enhance the event log architecture&amp;rdquo; and &amp;ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the â€‹Common Event Expression (CEE).&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Unfortunately all of these tools have one or more of the following problems.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;First&lt;/em&gt;, it looks like many of these open source efforts have all been abandoned or put in hibernation, and haven&amp;rsquo;t been updated since 2012 or 2013. liblognrom did put out &lt;a href=&#34;http://www.liblognorm.com/news/&#34;&gt;a couple of updates&lt;/a&gt; in the past couple of years.&lt;/p&gt;

&lt;p&gt;It is understandable. Log parsing is &lt;strong&gt;BORING&lt;/strong&gt;. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second&lt;/em&gt;, many of these commercial and open source tools uses regular expression to parse log messages. This approach is widely adopted because regular expression (regex) is a known quantity. Many administrators already know regex to some extend, and tools are widely available to interpret regex. In the early days of log analysis, Perl was used most often, so most rules you see are written in PCRE, or Perl Compatible Regular Expression. However, the process of writing regex rules is long, frustrating, and error-prone.&lt;/p&gt;

&lt;p&gt;Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Third&lt;/em&gt;, even after the regex rules are written, the performance is far from acceptable. This is mainly due to the fact that there&amp;rsquo;s no way to match multiple regular expressions at the same time. The engine would have to go through each individual rule separately. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.&lt;/p&gt;

&lt;p&gt;To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the analysts to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Last but not least&lt;/em&gt;, none of the existing tools can help analysts determine what patterns to write in order to parse their log files. Large companies can sometimes generate hundreds of gigabytes, if not terabytes, of data and billions of log messages per day. Sometimes it will take a team of analysts hundreds of hours to comb through log files, develop regex rules, test for conflicts and then repeat this whole process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequence</title>
      <link>http://localhost:1313/manual/sequence/</link>
      <pubDate>Sat, 28 Feb 2015 18:48:24 -0800</pubDate>
      
      <guid>http://localhost:1313/manual/sequence/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;#&#34; class=&#34;image fit&#34;&gt;&lt;img src=&#34;http://localhost:1313/images/banner2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A new approach taken by &lt;code&gt;sequence&lt;/code&gt; breaks the art of extracting meaningful information from log messages into three parts: scanning, analyzing and parsing.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;sequence&lt;/code&gt;, the first critical function to understanding a log message is to convert it into a sequence of valid tokens, i.e., &lt;strong&gt;meaningful&lt;/strong&gt; character strings. This function is called a &lt;em&gt;scanner&lt;/em&gt;, but it can be called a &lt;em&gt;lexical analyzer&lt;/em&gt;, &lt;em&gt;lexer&lt;/em&gt;, or &lt;em&gt;tokenizer&lt;/em&gt;. The &lt;code&gt;sequence&lt;/code&gt; &lt;em&gt;scanner&lt;/em&gt; goes through log message sequentially while tokentizing each part of the message, without the use of regular expressions.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;analyzer&lt;/em&gt; takes all the &lt;em&gt;sequences&lt;/em&gt; returned by the &lt;em&gt;scanner&lt;/em&gt; and attempt to automatically write the parsing rules. While it&amp;rsquo;s impossible to automatically generate 100% of the rules, the goal is to get to 75% accuracy by leveraging hints provided by analysts.&lt;/p&gt;

&lt;p&gt;Finally, the &lt;em&gt;parser&lt;/em&gt; will take the rules generated by the analyzer (and updated/corrected by some human) as well as the &lt;em&gt;sequences&lt;/em&gt; returned by the &lt;em&gt;scanner&lt;/em&gt; for log messages, and attempt to identify the matching rule. Based on the matching rule, the &lt;em&gt;parser&lt;/em&gt; will tag all the recognized tokens with a semantic tag, as defined by the user.&lt;/p&gt;

&lt;p&gt;The approach taken by the &lt;em&gt;parser&lt;/em&gt; is a tree-based parsing approach, which is also used by &lt;a href=&#34;http://www.liblognorm.com/&#34;&gt;liblognorm&lt;/a&gt;. This approach is much higher performance, as it only requires going through the log message only once. And because most, if not all, parsing rules are pre-built into a prefix tree, the engine just needs to walk through the tree once and be able to determine if there&amp;rsquo;s a match.&lt;/p&gt;

&lt;p&gt;Keep in mind that &lt;code&gt;sequence&lt;/code&gt; is NOT a replacement for Splunk or ElasticSearch, or any of the log analytics solutions. It is a library and a tool that can help parse messages much faster. The result of &lt;code&gt;sequence&lt;/code&gt; can be fed into these log analytics tools for further analysis.&lt;/p&gt;

&lt;h3 id=&#34;other-concepts:196ac21fc7c80b3643c5060d84767136&#34;&gt;Other Concepts&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;scanner&lt;/em&gt; returns a &lt;em&gt;sequence&lt;/em&gt;, or a list of tokens, which can then be consumed by the &lt;em&gt;analyzer&lt;/em&gt; or &lt;em&gt;parser&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, &lt;em&gt;Value&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;

&lt;h3 id=&#34;performance:196ac21fc7c80b3643c5060d84767136&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Most of the tools out there perform at several thousands messages per second (MPS) on a single machine using regex parsers. For &lt;code&gt;sequence&lt;/code&gt;, we are able to achieve over 100,000 MPS with average message size of 140 bytes, using a single CPU core. &lt;code&gt;sequence&lt;/code&gt; can also scale almost linearly (with a slight penalty due to communications overhead) as more cores are added.&lt;/p&gt;

&lt;p&gt;The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench parse -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench parse -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec

  $ ./sequence bench parse -d ../patterns -i ../data/asasshsudo.log
  Parsed 447745 messages in 4.47 secs, ~ 100159.65 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench parse -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench parse -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench parse -d ../patterns -i ../data/asasshsudo.log -w 2
  Parsed 447745 messages in 2.52 secs, ~ 177875.94 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&amp;rsquo;s only 1,200 MPS. However, some larger organizations I know are generating 60GB of Bluecoat logs per day, &lt;strong&gt;8 years ago&lt;/strong&gt;!! That&amp;rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&amp;rsquo;s over 10,000 MPS today. This already exceeds the capability of some parsers.&lt;/p&gt;

&lt;p&gt;To run through another example, &lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day&lt;/a&gt;. That&amp;rsquo;s 16,200 messages per second, and about 714 bytes per message. These EMC numbers are from 2013, so they have most likely increased by now.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can parse 37,000 MPS for messages averaging 714 bytes. That&amp;rsquo;s twice of what&amp;rsquo;s needed for EMC, using a SINGLE CORE!&lt;/p&gt;

&lt;p&gt;Obviously one can throw more hardware at solving the scale problem with the existing tools, but then again, why do that if you don&amp;rsquo;t need to. Just because you have the hardware doesn&amp;rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scanner</title>
      <link>http://localhost:1313/manual/scanner/</link>
      <pubDate>Sat, 28 Feb 2015 18:48:24 -0800</pubDate>
      
      <guid>http://localhost:1313/manual/scanner/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - &lt;a href=&#34;http://en.wikipedia.org/wiki/Lexical_analysis&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In &lt;code&gt;sequence&lt;/code&gt;, the first critical function to understanding a log message is to convert it into a sequence of valid tokens, i.e., &lt;strong&gt;meaningful&lt;/strong&gt; character strings. This function is called a &lt;em&gt;scanner&lt;/em&gt;, but it can be called a &lt;em&gt;lexical analyzer&lt;/em&gt;, &lt;em&gt;lexer&lt;/em&gt;, or &lt;em&gt;tokenizer&lt;/em&gt;. The &lt;code&gt;sequence&lt;/code&gt; &lt;em&gt;scanner&lt;/em&gt; goes through log message sequentially while tokentizing each part of the message, without the use of regular expressions.&lt;/p&gt;

&lt;p&gt;The biggest challenge to tokenizing is knowing where the break points are. Most log messages are free-form text, which means there&amp;rsquo;s no common structure to them. A log message &lt;em&gt;scanner&lt;/em&gt; or &lt;em&gt;tokenizer&lt;/em&gt; (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into &lt;strong&gt;meaningful&lt;/strong&gt; tokens.&lt;/p&gt;

&lt;p&gt;There are two reasons for the emphasis on &lt;strong&gt;meaningful&lt;/strong&gt;. First, due to the unstructured nature of most log messages, a scanner cannot depend on whitespaces to separate meaningful parts of the message. As an example, most timestamps in log messages have whitespaces, such as &lt;code&gt;01/02 03:04:05PM &#39;06 -0700&lt;/code&gt;. This semantically should be a single token. But if a scanner tokenizes based on whitespace, then it will be broken into 4 differnt parts, including &lt;code&gt;01/02&lt;/code&gt;, &lt;code&gt;03:04:05PM&lt;/code&gt;, &lt;code&gt;&#39;06&lt;/code&gt; and &lt;code&gt;-0700&lt;/code&gt;. This would obviously be wrong.&lt;/p&gt;

&lt;p&gt;A scanner also cannot depend on punctuations to tokenize the log message. In the above timestamp example, if you count &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;:&lt;/code&gt; as punctuations, then you would end up with 4 different tokens, including &lt;code&gt;01&lt;/code&gt;, &lt;code&gt;02 03&lt;/code&gt;, &lt;code&gt;04&lt;/code&gt;, and &lt;code&gt;05PM &#39;06 -0700&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Same if you were to look at an IPv4 address such as &amp;ldquo;127.0.0.1&amp;rdquo;, or IPv6 address such as &amp;ldquo;f0f0:f::1&amp;rdquo;, or MAC address such as &amp;ldquo;00:04:c1:8b:d8:82&amp;rdquo;, or a hex signature such as &amp;ldquo;de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56&amp;rdquo;. All these should be their own tokens. But if a scanner depends on only whitespace or punctuations to tokenize a message, these valid tokens would all be broken apart.&lt;/p&gt;

&lt;p&gt;Second, understanding the types of tokens will help the &lt;em&gt;parser&lt;/em&gt; distinguish between different log messages. If two messages are almost identical but one has a hostname and the other has an IP address, the &lt;em&gt;scanner&lt;/em&gt; can let the &lt;em&gt;parser&lt;/em&gt; know, and the &lt;em&gt;parser&lt;/em&gt; would the choose the correct pattern for the message.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; scanners currently automatically recognizes &lt;a href=&#34;http://localhost:1313/manual/tokens&#34;&gt;9 different token types&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/manual/timeformats&#34;&gt;42 different time stamps&lt;/a&gt; .&lt;/p&gt;

&lt;h3 id=&#34;scanners:adc8a12436d57d609395e4ca6900ba2b&#34;&gt;Scanners&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; implemented two scanners. First is a &lt;strong&gt;general scanner&lt;/strong&gt; which is a sequential lexical analyzer that breaks unstructured log messages into sequences of tokens. This scanner is mianly used for most system and network log messages that are free-form unstructured text.&lt;/p&gt;

&lt;p&gt;As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &amp;ldquo;;&amp;rdquo; or &amp;ldquo;:&amp;ldquo;, as they would break the log mesage into useless parts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;ts&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;string&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;string&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;string&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;string&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Second is a &lt;strong&gt;JSON scanner&lt;/strong&gt;, implemented by &lt;code&gt;ScanJson()&lt;/code&gt;, that scans JSON messages and convert them into a &lt;em&gt;sequence&lt;/em&gt; that can be parsed by the &lt;em&gt;parser&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ScanJson()&lt;/code&gt; will flatten a json string into key=value pairs, and it performs the following transformation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;all {, }, [, ], &amp;ldquo;, characters are removed&lt;/li&gt;
&lt;li&gt;colon between key and value are changed to &amp;ldquo;=&amp;rdquo;&lt;/li&gt;
&lt;li&gt;nested objects have their keys concatenated with &amp;ldquo;.&amp;rdquo;, so a json string like &lt;code&gt;&amp;quot;userIdentity&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;IAMUser&amp;quot;}&lt;/code&gt; will be returned as &lt;code&gt;userIdentity.type=IAMUser&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;arrays are flattened by appending an index number to the end of the key, starting with 0, so a json string like &lt;code&gt;{&amp;quot;value&amp;quot;:[{&amp;quot;open&amp;quot;:&amp;quot;2014-08-16T13:00:00.000+0000&amp;quot;}]}&lt;/code&gt; will be returned as &lt;code&gt;value.0.open=2014-08-16T13:00:00.000+0000&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;skips any key that has an empty value, so json strings like &lt;code&gt;&amp;quot;reference&amp;quot;:&amp;quot;&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;filterSet&amp;quot;: {}&lt;/code&gt; will not show up in the Sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;EventTime&amp;quot;:&amp;quot;2014-08-16T12:45:03-0400&amp;quot;,&amp;quot;URI&amp;quot;:&amp;quot;myuri&amp;quot;,&amp;quot;uri_payload&amp;quot;:{&amp;quot;value&amp;quot;:[{&amp;quot;open&amp;quot;:&amp;quot;2014-08-16T13:00:00.000+0000&amp;quot;,&amp;quot;close&amp;quot;:&amp;quot;2014-08-16T23:00:00.000+0000&amp;quot;,&amp;quot;isOpen&amp;quot;:true,&amp;quot;date&amp;quot;:&amp;quot;2014-08-16&amp;quot;}],&amp;quot;Count&amp;quot;:1}}

  #   0: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;EventTime&amp;quot; }
  #   1: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   2: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;time&amp;quot;, Value=&amp;quot;2014-08-16T12:45:03-0400&amp;quot; }
  #   3: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;URI&amp;quot; }
  #   4: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   5: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;myuri&amp;quot; }
  #   6: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;uri_payload.value.0.open&amp;quot; }
  #   7: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;time&amp;quot;, Value=&amp;quot;2014-08-16T13:00:00.000+0000&amp;quot; }
  #   9: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;uri_payload.value.0.close&amp;quot; }
  #  10: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  11: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;time&amp;quot;, Value=&amp;quot;2014-08-16T23:00:00.000+0000&amp;quot; }
  #  12: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;uri_payload.value.0.isOpen&amp;quot; }
  #  13: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  14: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;true&amp;quot; }
  #  15: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;uri_payload.value.0.date&amp;quot; }
  #  16: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  17: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;time&amp;quot;, Value=&amp;quot;2014-08-16&amp;quot; }
  #  18: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;uri_payload.Count&amp;quot; }
  #  19: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;literal&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;funknown&amp;quot;, Type=&amp;quot;integer&amp;quot;, Value=&amp;quot;1&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;design:adc8a12436d57d609395e4ca6900ba2b&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;sequence&lt;/code&gt; scanner, there are three FSMs: Time, HexString and General.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Time FSM understands &lt;a href=&#34;http://localhost:1313/manual/timeformats&#34;&gt;42 different time stamps&lt;/a&gt;. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.&lt;/li&gt;
&lt;li&gt;The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).&lt;/li&gt;
&lt;li&gt;The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each character in the log message are run through all three FSMs, and the following logics are applied:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If a time format is matched, that&amp;rsquo;s what it will be returned.&lt;/li&gt;
&lt;li&gt;Next if a hex string is matched, it is also returned.

&lt;ul&gt;
&lt;li&gt;We mark anything with 5 colon characters and no successive colons like &amp;ldquo;::&amp;rdquo; to be a MAC address.&lt;/li&gt;
&lt;li&gt;Anything that has 7 colons and no successive colons are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Anything that has less than 7 colons but has only 1 set of successive colons like &amp;ldquo;::&amp;rdquo; are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Everything else is just a literal.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finally if neither of the above matched, we return what the general FSM has matched.

&lt;ul&gt;
&lt;li&gt;The general FSM recognizes these quote characters: &amp;ldquo;, &amp;lsquo; and &amp;lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.&lt;/li&gt;
&lt;li&gt;Anything that starts with http:// or https:// are considered URLs.&lt;/li&gt;
&lt;li&gt;Anything that matches 4 integer octets are considered IP addresses.&lt;/li&gt;
&lt;li&gt;Anything that matches two integers with a dot in between are considered floats.&lt;/li&gt;
&lt;li&gt;Anything that matches just numbers are considered integers.&lt;/li&gt;
&lt;li&gt;Everything else are literals.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To achieve the performance we want, &lt;code&gt;sequence&lt;/code&gt; took great pain to go through the log message once and only once. This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; also took great pain to ensure that there&amp;rsquo;s no need to look forward or look backward in the log message to determine the current token type. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.&lt;/p&gt;

&lt;p&gt;In reality though, while the scanners only looping through the log string once, and only once, it does run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM.&lt;/p&gt;

&lt;p&gt;You can &lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;read more details&lt;/a&gt; on how the scanners were able to achieve the performance.&lt;/p&gt;

&lt;h3 id=&#34;performance:adc8a12436d57d609395e4ca6900ba2b&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; &lt;em&gt;scanner&lt;/em&gt; is able to tokenize almost 200,000 messages per second for messages averaging 136 bytes. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzer</title>
      <link>http://localhost:1313/manual/analyzer/</link>
      <pubDate>Sat, 28 Feb 2015 18:48:24 -0800</pubDate>
      
      <guid>http://localhost:1313/manual/analyzer/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;#&#34; class=&#34;image fit&#34;&gt;&lt;img src=&#34;http://localhost:1313/images/pic07.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This section will go through additional details of how the &lt;code&gt;sequence&lt;/code&gt; analyzer reduces 100 of 1000&amp;rsquo;s of raw log messages down to just 10&amp;rsquo;s of unique patterns, and then determining how to label the individual tokens. The goal is to reduce the time to write parsing rules by 50-75%.&lt;/p&gt;

&lt;p&gt;Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages. By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the output file has entries such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&amp;rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;em&gt;analyzer&lt;/em&gt; is performed in two separate steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Identifying the unique patterns&lt;/li&gt;
&lt;li&gt;Determining the correct fields&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;identifying-the-unique-patterns:029e5400481933e044356e0e58bbed76&#34;&gt;Identifying the Unique Patterns&lt;/h3&gt;

&lt;p&gt;Analyzer builds an analysis tree that represents all the &lt;em&gt;sequences&lt;/em&gt; from the &lt;em&gt;scanners&lt;/em&gt;. It can be used to determine all of the unique patterns for a large body of messages.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&amp;rsquo;s something we can extract. For example, take a look at the following two messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first token of each message is a timestamp, and the 3rd token of each message is the literal &amp;ldquo;sshd&amp;rdquo;. For the literals &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &amp;ldquo;sshd&amp;rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo; happens to
represent the syslog host.&lt;/p&gt;

&lt;p&gt;Looking further down the message, the literals &amp;ldquo;password&amp;rdquo; and &amp;ldquo;publickey&amp;rdquo; also share a common parent, &amp;ldquo;Accepted&amp;rdquo;, and a common child, &amp;ldquo;for&amp;rdquo;. So that means the token in this position is also a variable token (of type TokenString).&lt;/p&gt;

&lt;p&gt;You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If later we add another message to this mix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Analyzer will determine that the literals &amp;ldquo;Accepted&amp;rdquo; in the 1st message, and &amp;ldquo;Failed&amp;rdquo; in the 3rd message share a common parent &amp;ldquo;:&amp;rdquo; and a common child &amp;ldquo;password&amp;rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By applying this concept, we can effectively identify all the unique patterns in a log file.&lt;/p&gt;

&lt;h3 id=&#34;determining-the-correct-fields:029e5400481933e044356e0e58bbed76&#34;&gt;Determining the Correct Fields&lt;/h3&gt;

&lt;p&gt;Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.&lt;/p&gt;

&lt;p&gt;System and network logs are mostly free form text. There&amp;rsquo;s no specific patterns to any of them. So it&amp;rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no &amp;ldquo;machine learning&amp;rdquo; here. This section is all about codifying these human learnings. I&amp;rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.&lt;/p&gt;

&lt;h4 id=&#34;0-parsing-email-and-hostname-formats:029e5400481933e044356e0e58bbed76&#34;&gt;0. Parsing Email and Hostname Formats&lt;/h4&gt;

&lt;p&gt;This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&amp;rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&amp;rsquo;t care about performance as much, we can do this as post-processing step.&lt;/p&gt;

&lt;p&gt;To recognize the hostname, we try to match the &amp;ldquo;effective TLD&amp;rdquo; using the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;xparse/etld&lt;/a&gt; package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from &lt;a href=&#34;https://www.publicsuffix.org/list/effective_tld_names.dat&#34;&gt;https://www.publicsuffix.org/list/effective_tld_names.dat&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;1-recognizing-syslog-headers:029e5400481933e044356e0e58bbed76&#34;&gt;1. Recognizing Syslog Headers&lt;/h4&gt;

&lt;p&gt;First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	// RFC5424
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&amp;quot;
	// - &amp;quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&amp;quot;
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&amp;quot;
	// RFC3164
	// - &amp;quot;Oct 11 22:14:15 mymachine su: ...&amp;quot;
	// - &amp;quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&amp;quot;
	// - &amp;quot;jan 12 06:49:56 irc last message repeated 6 times&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.&lt;/p&gt;

&lt;h4 id=&#34;2-marking-key-and-value-pairs:029e5400481933e044356e0e58bbed76&#34;&gt;2. Marking Key and Value Pairs&lt;/h4&gt;

&lt;p&gt;The next step we perform is to mark known &amp;ldquo;keys&amp;rdquo;. There are two types of keys. First, we identify any token before the &amp;ldquo;=&amp;rdquo; as a key. For example, the message &lt;code&gt;fw=TOPSEC priv=6 recorder=kernel type=conn&lt;/code&gt; contains 4 keys: &lt;code&gt;fw&lt;/code&gt;, &lt;code&gt;priv&lt;/code&gt;, &lt;code&gt;recorder&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.&lt;/p&gt;

&lt;p&gt;The second types of keys are determined by keywords that often appear in front of other tokens, I call these &lt;strong&gt;prekeys&lt;/strong&gt;. For example, we know that the prekey &lt;code&gt;from&lt;/code&gt; usually appears in front of any source host or IP address, and the prekey &lt;code&gt;to&lt;/code&gt; usually appears in front of any destination host or IP address. Below are some examples of these prekeys.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
port 		= [ &amp;quot;%srcport%&amp;quot;, &amp;quot;%dstport%&amp;quot; ]
proto		= [ &amp;quot;%protocol%&amp;quot; ]
sport		= [ &amp;quot;%srcport%&amp;quot; ]
src 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
to 			= [ &amp;quot;%dsthost%&amp;quot;, &amp;quot;%dstipv4%&amp;quot;, &amp;quot;%dstuser%&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-labeling-values-by-their-keys:029e5400481933e044356e0e58bbed76&#34;&gt;3. Labeling &amp;ldquo;Values&amp;rdquo; by Their Keys&lt;/h4&gt;

&lt;p&gt;Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both &lt;code&gt;key=value&lt;/code&gt; or &lt;code&gt;key=&amp;quot;value&amp;quot;&lt;/code&gt; formats (or other quote characters like &amp;lsquo; or &amp;lt;).&lt;/p&gt;

&lt;p&gt;For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as &lt;code&gt;from 192.168.1.1&lt;/code&gt; and &lt;code&gt;from ip 192.168.1.1&lt;/code&gt; will identify &lt;code&gt;192.168.1.1&lt;/code&gt; as the &lt;code&gt;%srcipv4%&lt;/code&gt; based on the above mapping, but we will miss &lt;code&gt;from ip address 192.168.1.1&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;4-identifying-known-keywords:029e5400481933e044356e0e58bbed76&#34;&gt;4. Identifying Known Keywords&lt;/h4&gt;

&lt;p&gt;Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;action = [
	&amp;quot;access&amp;quot;,
	&amp;quot;alert&amp;quot;,
	&amp;quot;allocate&amp;quot;,
	&amp;quot;allow&amp;quot;,
	.
	.
	.
]

status = [
	&amp;quot;accept&amp;quot;,
	&amp;quot;error&amp;quot;,
	&amp;quot;fail&amp;quot;,
	&amp;quot;failure&amp;quot;,
	&amp;quot;success&amp;quot;
]

object = [
	&amp;quot;account&amp;quot;,
	&amp;quot;app&amp;quot;,
	&amp;quot;bios&amp;quot;,
	&amp;quot;driver&amp;quot;,
	.
	.
	.
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a &lt;a href=&#34;https://github.com/surge/porter2&#34;&gt;porter2 stemming operation&lt;/a&gt; on the literal, then compare to the above list (which is also porter2 stemmed).&lt;/p&gt;

&lt;p&gt;If a literal matches one of the above lists, then the corresponding label (&lt;code&gt;action&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;object&lt;/code&gt;, &lt;code&gt;srcuser&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, or &lt;code&gt;protocol&lt;/code&gt;) is applied.&lt;/p&gt;

&lt;h4 id=&#34;5-determining-positions-of-specific-types:029e5400481933e044356e0e58bbed76&#34;&gt;5. Determining Positions of Specific Types&lt;/h4&gt;

&lt;p&gt;In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for &lt;code&gt;%time%&lt;/code&gt;, &lt;code&gt;%url%&lt;/code&gt;, &lt;code&gt;%mac%&lt;/code&gt;, &lt;code&gt;%ipv4%&lt;/code&gt;, &lt;code&gt;%host%&lt;/code&gt;, and &lt;code&gt;%email%&lt;/code&gt; tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first %time% token is labeled as %msgtime%&lt;/li&gt;
&lt;li&gt;The first %url% token is labeled as %object%&lt;/li&gt;
&lt;li&gt;The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%&lt;/li&gt;
&lt;li&gt;The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%&lt;/li&gt;
&lt;li&gt;The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%&lt;/li&gt;
&lt;li&gt;The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;6-scanning-for-ip-port-or-ip-port-pairs:029e5400481933e044356e0e58bbed76&#34;&gt;6. Scanning for ip/port or ip:port Pairs&lt;/h4&gt;

&lt;p&gt;Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &amp;ldquo;/&amp;rdquo; or &amp;ldquo;:&amp;ldquo;. Then we label these numbers as either &lt;code&gt;%srcport%&lt;/code&gt; or &lt;code&gt;%dstport%&lt;/code&gt; based on how the previous IP address is labeled.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parser</title>
      <link>http://localhost:1313/manual/parser/</link>
      <pubDate>Sat, 28 Feb 2015 18:48:24 -0800</pubDate>
      
      <guid>http://localhost:1313/manual/parser/</guid>
      <description>&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; &lt;em&gt;parser&lt;/em&gt; takes the rules generated by the analyzer (and updated/corrected by some human) as well as the &lt;em&gt;sequences&lt;/em&gt; returned by the &lt;em&gt;scanner&lt;/em&gt; for log messages, and will identify the matching rule. Based on the matching rule, the &lt;em&gt;parser&lt;/em&gt; will tag all the recognized tokens with a semantic tag, as defined by the user.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>